{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipgraELuiBcc","outputId":"d7a72501-2dec-48c4-bccd-2b97eb3fd42f","executionInfo":{"status":"ok","timestamp":1685109565439,"user_tz":-330,"elapsed":6470,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n"]}],"source":["pip install opencv-python"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"i3CBEWXhFiIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685110074718,"user_tz":-330,"elapsed":509290,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}},"outputId":"a7877e72-b415-4859-efbb-16f646815ad6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! pip install kaggle"],"metadata":{"id":"XdMO9UbXi-dS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685110078939,"user_tz":-330,"elapsed":4247,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}},"outputId":"c4d8db83-9d6e-4a11-806d-54c4586ae461"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"]}]},{"cell_type":"code","source":["! mkdir ~/.kaggle"],"metadata":{"id":"v1PeXdQui-gp","executionInfo":{"status":"ok","timestamp":1685110078940,"user_tz":-330,"elapsed":31,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["! cp kaggle.json ~/.kaggle/"],"metadata":{"id":"H0QWrxtzjFgw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685110078941,"user_tz":-330,"elapsed":29,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}},"outputId":"217b1718-e803-4a23-9c4e-63fb48213200"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat 'kaggle.json': No such file or directory\n"]}]},{"cell_type":"code","source":["! chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"Kv8-ncN1jFdF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685110078942,"user_tz":-330,"elapsed":20,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}},"outputId":"3db2928c-510f-4a89-f740-e8c7726e244c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"]}]},{"cell_type":"code","source":["! kaggle datasets download vipoooool/new-plant-diseases-dataset"],"metadata":{"id":"u2a4Pvl-jFak","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685110079698,"user_tz":-330,"elapsed":769,"user":{"displayName":"Yash Khilavdiya","userId":"01579921804452099851"}},"outputId":"50c0564e-1f68-426e-d487-b4169b51cb64"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 5, in <module>\n","    from kaggle.cli import main\n","  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n","    api.authenticate()\n","  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n","    raise IOError('Could not find {}. Make sure it\\'s located in'\n","OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"]}]},{"cell_type":"markdown","source":["# Download and Input Dataset"],"metadata":{"id":"PYyPkWETqGJp"}},{"cell_type":"code","source":["import zipfile\n","with zipfile.ZipFile(\"/content/drive/MyDrive/Innovative Project - Plant Disease Detector/archive.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall()"],"metadata":{"id":"ZjHLY4Y_j0tt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"XESVrc3-O7SD"}},{"cell_type":"code","source":["import cv2\n","import numpy as np"],"metadata":{"id":"dOiitsqCiCZt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src = cv2.imread(\"/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train/Tomato___Septoria_leaf_spot/002533c1-722b-44e5-9d2e-91f7747b2543___Keller.St_CG 1831.JPG\", 1) # read input image\n","imshow(src)"],"metadata":{"id":"nYzytDG4iGh6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convex Hull preprocessing on OSTU threshold"],"metadata":{"id":"YCoAmTBspj9L"}},{"cell_type":"code","source":["from pylab import imshow,show\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY) # convert to grayscale\n","\n","blur = cv2.blur(gray, (3, 3)) # blur the image\n","\n","ret, thresh = cv2.threshold(blur, 50, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n","imshow(src)\n","imshow(blur)\n"],"metadata":{"id":"hHWvasNvkzpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)"],"metadata":{"id":"iGNkB_vvk5JC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create hull array for convex hull points\n","\n","hull = []\n","\n"," \n","\n","# calculate points for each contour\n","\n","for i in range(len(contours)):\n","\n","    # creating convex hull object for each contour\n","\n","    hull.append(cv2.convexHull(contours[i], False))\n","    \n"],"metadata":{"id":"2C9-o3nPk92P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","drawing = np.zeros((thresh.shape[0], thresh.shape[1], 3), np.uint8)\n","\n"," # draw contours and hull points\n","\n","for i in range(len(contours)):\n","\n","    color_contours = (0, 255, 0) # green - color for contours\n","\n","    color = (0, 0, 255) # blue - color for convex hull\n","\n","    # draw ith contour\n","\n","    cv2.drawContours(drawing, contours, i, color_contours, 1, 8, hierarchy)\n","\n","    # draw ith convex hull object\n","\n","    imshow(cv2.drawContours(drawing, hull, i, color, 1, 8))\n","#imshow(drawing)\n","#imshow(src)"],"metadata":{"id":"aXt3wc1RlBer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install mahotas"],"metadata":{"id":"Lhsquf0jlEzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Extraction using SURF"],"metadata":{"id":"LBf4YlX1ptYi"}},{"cell_type":"code","source":["import mahotas\n","#import mahotas.demos\n","import mahotas as mh\n","import numpy as np\n","from pylab import imshow, show\n","from mahotas.features import surf\n","\n","nuclear = src\n"," \n","# filtering image\n","nuclear = nuclear[:, :, 0]\n"," \n","# adding gaussian filter\n","nuclear = mahotas.gaussian_filter(nuclear, 4)\n"," \n","# showing image\n","print(\"Image\")\n","imshow(nuclear)\n","show()\n","spoints = surf.surf(nuclear)"],"metadata":{"id":"dTe0qoCulIzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#classification"],"metadata":{"id":"56Nu8RTerp4U"}},{"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import datasets, layers, models, losses, Model"],"metadata":{"id":"B8eEcRtMlMH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (32,32,3))\n","x_train = tf.pad(src, [[0, 0], [2,2], [2,2]])/255\n","for layer in base_model.layers:\n","    layer.trainable = False"],"metadata":{"id":"QkRoNt0nlRL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","x = layers.Flatten()(base_model.output)\n","x = layers.Dense(1000, activation='relu')(x)\n","predictions = layers.Dense(10, activation = 'softmax')(x)\n","predictions"],"metadata":{"id":"B41vkNyzlURf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torchsummary"],"metadata":{"id":"HZnjNr6ylcSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os                       # for working with files\n","import numpy as np              # for numerical computationss\n","import pandas as pd             # for working with dataframes\n","import torch                    # Pytorch module \n","import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\n","import torch.nn as nn           # for creating  neural networks\n","from torch.utils.data import DataLoader # for dataloaders \n","from PIL import Image           # for checking images\n","import torch.nn.functional as F # for functions for calculating loss\n","import torchvision.transforms as transforms   # for transforming images into tensors \n","from torchvision.utils import make_grid       # for data checking\n","from torchvision.datasets import ImageFolder  # for working with classes and images\n","from torchsummary import summary              # for getting the summary of our model\n","\n","%matplotlib inline"],"metadata":{"id":"5F-8IegmlnZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = \"/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)\"\n","train_dir = data_dir + \"/train\"\n","valid_dir = data_dir + \"/valid\"\n","diseases = os.listdir(train_dir)"],"metadata":{"id":"do5F9cPslp_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(diseases)"],"metadata":{"id":"GxkARp3Ul4NO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plants = []\n","NumberOfDiseases = 0\n","for plant in diseases:\n","    if plant.split('___')[0] not in plants:\n","        plants.append(plant.split('___')[0])\n","    if plant.split('___')[1] != 'healthy':\n","        NumberOfDiseases += 1"],"metadata":{"id":"H4CaIGjil8g3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plants"],"metadata":{"id":"VoKvOb10l-GA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print(\"Number of plants: {}\".format(len(plants)))"],"metadata":{"id":"v4URU40Fl-DA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print(\"Number of diseases: {}\".format(NumberOfDiseases))"],"metadata":{"id":"zSwzEwg7l-A2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of images for each disease\n","nums = {}\n","for disease in diseases:\n","    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n","    \n","# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n","\n","img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n","img_per_class"],"metadata":{"id":"hoiDxJ6hl9_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting number of images available for each disease\n","index = [n for n in range(38)]\n","plt.figure(figsize=(20, 5))\n","plt.bar(index, [n for n in nums.values()], width=0.3)\n","plt.xlabel('Plants/Diseases', fontsize=10)\n","plt.ylabel('No of images available', fontsize=10)\n","plt.xticks(index, diseases, fontsize=5, rotation=90)\n","plt.title('Images per each class of plant disease')"],"metadata":{"id":"4tPih-fNl99D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train = ImageFolder(train_dir, transform=transforms.ToTensor())\n","valid = ImageFolder(valid_dir, transform=transforms.ToTensor()) "],"metadata":{"id":"jG1_pnfymPeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = train[1]"],"metadata":{"id":"gHKuaG-MmPa1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def show_image(image, label):\n","    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n","    plt.imshow(image.permute(1, 2, 0))"],"metadata":{"id":"S7zxKr-HmPYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_image(*train[1])"],"metadata":{"id":"OYu2x4WBmPXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_image(*train[70000])"],"metadata":{"id":"owJ_6Pd5mPVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting the seed value\n","random_seed = 7\n","torch.manual_seed(random_seed)"],"metadata":{"id":"T9WewmyCl962"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting the batch size\n","batch_size = 32"],"metadata":{"id":"AmjBFUfXl94n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helper function to show a batch of training instances\n","def show_batch(data):\n","    for images, labels in data:\n","        fig, ax = plt.subplots(figsize=(30, 30))\n","        ax.set_xticks([]); ax.set_yticks([])\n","        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n","        break"],"metadata":{"id":"FQIy9K0ABbPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Images for first batch of training\n","show_batch(train_dl) "],"metadata":{"id":"0sgRuro8mw4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DataLoaders for training and validation\n","train_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","valid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)"],"metadata":{"id":"yD7ZSyDSl92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4O3m7zeKl90B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for moving data into GPU (if available)\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available:\n","        return torch.device(\"cuda\")\n","    else:\n","        return torch.device(\"cpu\")\n","\n","# for moving data to device (CPU or GPU)\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","# for loading in the device (GPU if available else CPU)\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl:\n","            yield to_device(b, self.device)\n","        \n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"],"metadata":{"id":"CMo_JG-xmw1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = get_default_device()\n","device"],"metadata":{"id":"kOhI6p00mwy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Moving data into GPU\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)"],"metadata":{"id":"OnZNaVKVmww8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SimpleResidualBlock(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n","        self.relu2 = nn.ReLU()\n","        \n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.relu1(out)\n","        out = self.conv2(out)\n","        return self.relu2(out) + x # ReLU can be applied before or after adding the input"],"metadata":{"id":"grsU_D29mwvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for calculating the accuracy\n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","\n","\n","# base class for the model\n","class ImageClassificationBase(nn.Module):\n","    \n","    def training_step(self, batch):\n","        images, labels = batch\n","        out = self(images)                  # Generate predictions\n","        loss = F.cross_entropy(out, labels) # Calculate loss\n","        return loss\n","    \n","    def validation_step(self, batch):\n","        images, labels = batch\n","        out = self(images)                   # Generate prediction\n","        loss = F.cross_entropy(out, labels)  # Calculate loss\n","        acc = accuracy(out, labels)          # Calculate accuracy\n","        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n","    \n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x[\"val_loss\"] for x in outputs]\n","        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss  \n","        epoch_accuracy = torch.stack(batch_accuracy).mean()\n","        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy} # Combine accuracies\n","    \n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))\n","        "],"metadata":{"id":"KlvrDmUBmwtB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Architecture for training\n","\n","# convolution block with BatchNormalization\n","def ConvBlock(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","             nn.BatchNorm2d(out_channels),\n","             nn.ReLU(inplace=True)]\n","    if pool:\n","        layers.append(nn.MaxPool2d(4))\n","    return nn.Sequential(*layers)\n","\n","\n","# resnet architecture \n","class ResNet9(ImageClassificationBase):\n","    def __init__(self, in_channels, num_diseases):\n","        super().__init__()\n","        \n","        self.conv1 = ConvBlock(in_channels, 64)\n","        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 \n","        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n","        \n","        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16\n","        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44\n","        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n","        \n","        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n","                                       nn.Flatten(),\n","                                       nn.Linear(512, num_diseases))\n","        \n","    def forward(self, xb): # xb is the loaded batch\n","        out = self.conv1(xb)\n","        out = self.conv2(out)\n","        out = self.res1(out) + out\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.res2(out) + out\n","        out = self.classifier(out)\n","        return out        "],"metadata":{"id":"R8prf3J5mwqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining the model and moving it to the GPU\n","model = to_device(ResNet9(3, len(train.classes)), device) \n","model"],"metadata":{"id":"jQEw01ZonOEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# getting summary of the model\n","INPUT_SHAPE = (3, 256, 256)\n","print(summary(model.cuda(), (INPUT_SHAPE)))"],"metadata":{"id":"EQGNtj1ZnOBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for training\n","@torch.no_grad()\n","def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.validation_epoch_end(outputs)\n","\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","    \n","\n","def fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n","                grad_clip=None, opt_func=torch.optim.SGD):\n","    torch.cuda.empty_cache()\n","    history = []\n","    \n","    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n","    # scheduler for one cycle learniing rate\n","    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n","    \n","    \n","    for epoch in range(epochs):\n","        # Training\n","        model.train()\n","        train_losses = []\n","        lrs = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            \n","            # gradient clipping\n","            if grad_clip: \n","                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n","                \n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","            # recording and updating learning rates\n","            lrs.append(get_lr(optimizer))\n","            sched.step()\n","            \n","    \n","        # validation\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        result['lrs'] = lrs\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","        \n","    return history\n","    "],"metadata":{"id":"hmjttIZznN_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","history = [evaluate(model, valid_dl)]\n","history"],"metadata":{"id":"PQy-EuOWnN9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 2\n","max_lr = 0.01\n","grad_clip = 0.1\n","weight_decay = 1e-4\n","opt_func = torch.optim.Adam"],"metadata":{"id":"mK3II8ovnN6r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","history += fit_OneCycle(epochs, max_lr, model, train_dl, valid_dl, \n","                             grad_clip=grad_clip, \n","                             weight_decay=1e-4, \n","                             opt_func=opt_func)"],"metadata":{"id":"MyOr-_PQnN4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_accuracies(history):\n","    accuracies = [x['val_accuracy'] for x in history]\n","    plt.plot(accuracies, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.title('Accuracy vs. No. of epochs');\n","    plt.legend('Training',loc='upper left')\n","\n","def plot_losses(history):\n","    train_losses = [x.get('train_loss') for x in history]\n","    val_losses = [x['val_loss'] for x in history]\n","    plt.plot(train_losses, '-bx')\n","    plt.plot(val_losses, '-rx')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(['Training', 'Validation'])\n","    plt.title('Loss vs. No. of epochs');\n","    \n","def plot_lrs(history):\n","    lrs = np.concatenate([x.get('lrs', []) for x in history])\n","    plt.plot(lrs)\n","    plt.xlabel('Batch no.')\n","    plt.ylabel('Learning rate')\n","    plt.title('Learning Rate vs. Batch no.');\n","    plt.legend('Training',loc='upper left')"],"metadata":{"id":"paQrEbYJnN2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracies(history)"],"metadata":{"id":"XPkCgvganN0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_lrs(history)"],"metadata":{"id":"sWmAPEk5nNyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dir = \"/content/test\"\n","test = ImageFolder(test_dir, transform=transforms.ToTensor())"],"metadata":{"id":"FItWSMaFnNvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_images = sorted(os.listdir(test_dir + '/test')) # since images in test folder are in alphabetical order\n","test_images"],"metadata":{"id":"exK3AmoNnNtZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_image(img, model):\n","    \"\"\"Converts image to array and return the predicted class\n","        with highest probability\"\"\"\n","    # Convert to a batch of 1\n","    xb = to_device(img.unsqueeze(0), device)\n","    # Get predictions from model\n","    yb = model(xb)\n","    # Pick index with highest probability\n","    _, preds  = torch.max(yb, dim=1)\n","    # Retrieve the class label\n","\n","    return train.classes[preds[0].item()]"],"metadata":{"id":"OeDFMJ0gmwok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n","import numpy as np"],"metadata":{"id":"7XRR8cfY4Rsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predicting first image\n","img, label = test[0]\n","plt.imshow(img.permute(1, 2, 0))\n","print('Label:', test_images[0], ', Predicted:', predict_image(img, model))"],"metadata":{"id":"CaQPeBWtpT7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# getting all predictions (actual label vs predicted)\n","for i, (img, label) in enumerate(test):\n","    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))"],"metadata":{"id":"gYB-_hnYpT3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y_pred=model.predict(test_dl,axis=0)\n","# y_pred=np.argmax(y_pred,axis=1)"],"metadata":{"id":"NseD8tS1pTzD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_images_list = []\n","predict_image_list = []\n","\n","for i, (img, label) in enumerate(test):\n","  test_images_list.append(test_images)\n","  predict_image_list.append(predict_image(img, model))\n","\n","  \n","print(test_images_list)\n","print(predict_image_list)"],"metadata":{"id":"Zoj4jaYBpTwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"mekdOPU28WG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_image_2(img):\n","  im_3d = img.reshape(-1,256,256,3)\n","  prediction = model.predict(im_3d)[0]\n","  return {train.classes[i]: float(prediction[i]) for i in range(2)}"],"metadata":{"id":"WWdjg6AqpTue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = test[0]\n","pre = predict_image_2(img)\n","print(pre[0])"],"metadata":{"id":"hgWgJlZMpTsC"},"execution_count":null,"outputs":[]}]}